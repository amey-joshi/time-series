---
title: "Time-series-3"
author: "Amey Joshi"
date: "17/06/2019"
output: beamer_presentation
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(astsa)
library(tseries)
```

## Time series models
- We observed that values in a time series data are not independent of each other.
- We respect the principle of causality and assume that past values affect the
present value.
- How many past values should we consider?
  - One
  - A few
  - All
- Answer to this question decides the modelling technique.
- How do we answer the question? As usual, looking at the data may help.

## Dependence on past - 1
```{r}
lag1.plot(soi, 12)
```

## Dependence on past - 2
Notice that
- there is a strong positive correlation with lags $1, 2, 11$ and $12$.
- there is a moderate negative correlation with lags $6$ and $7$.

Remember that we are talking about correlation. We do not attempt to answer the
question "Why is there a correlation?".

We can do a similar visual analysis even two different time series. For instance,
we can find out if the number of fish has a correlation with SOI with a lag.

## Dependence on past - 3
```{r}
lag2.plot(soi, rec, 11)
```

## Dependence on past - 4
- There is a strong nonlinear correlation between the number of fish and SOI 
with lags $5, 6, 7, 8$ and $9$.
- SOI affects the number of fish with a lag of half a year.
- Visualization is a valuable tool to determine parameters of models we will
describe next.

If SOI of this this month has a correlation with SOI of $1, 2, 11$ and $12$
ago, can we build a regression model with the latter data as predictors? Can
we model a time series as a regression over its own past values?

This is the idea behind autoregressive (AR) models.

## Are SOI and # fish data time series stationary?
\tiny
```{r}
adf.test(soi)
```

\normalsize
The very small $p$ value suggests that we can reject the null hypothesis.
\tiny
```{r}
adf.test(rec)
```
\normalsize
The time series of number of fish too can be assumed to be stationary.

## What has stationarity to do with building a model?
We build an AR model based on statistical relation between a data point and the
past data points. If the statistical nature is going to change over time then our
model will no longer be valid.

It is a good idea to check stationarity while building AR models and also the
ones founded on similar ideas like the moving average (MA) models or a 
combination of the two, namely ARMA and ARIMA.

What if a time series is not stationary? There are other techniques like 
GARCH.

## $AR$ models - 1
A time series $x_t$ is modelled as
$$
x_t - \mu = \beta_1(x_{t-1} - \mu) + \epsilon_t,
$$
where $\alpha$ is a constant, $\mu$ is the mean of the process and $\epsilon_t$
is a stationary white noise. We rearrange the above equation to get
$$
x_t = \beta_0 + \beta_1 x_{t-1} + \epsilon_t,
$$
where $\beta_0 = (1 - \alpha)\mu$. This form brings out the regression structure
of the model. It is called $AR(1)$ because $x_t$ depends only on $x_{t-1}$.

An $AR(p)$ model is
$$
x_t = \beta_0 + \beta_1 x_{t-1} + \cdots + \beta_p x_{t-p} + \epsilon_t.
$$

## AR models - 2
AR models have an infinite memory. One can easily conclude that
$$
x_t = \beta_0(1 + \beta_1 + \cdots + \beta_1^p) + \beta_1^{p+1}x_{t - p - 1} + 
(\epsilon_t + \beta_1\epsilon_{t-1} + \cdots + \beta_1^p\epsilon_{t-p}).
$$

If $\beta_1 < 1$ then the influnece of past fades away for large $p$. 

An $AR(p)$ process is not always stationary. To determine if it is, we examine the
roots of the characteristic equation
$$
z^p - \beta_1 z^{p-1} - \cdots - \beta_p = 0.
$$
If all roots are inside the unit circle then the process is stationary. This is the
idea behind the Phillips-Perron unit root test.

## MA model - 1
A time series $x_t$ modelled as
$$
x_t - \mu = \epsilon_t - \theta\epsilon_{t-1}
$$
is called as $MA(1)$ process. It is easy to check that
$$
\rho(h) = \begin{cases}
-\frac{\theta}{1 + \theta^2} & h = 1 \\
0 & h > 1.
\end{cases}
$$
An $MA(p)$ process is modelled as
$$
x_t = \mu + \epsilon_t - \theta_1\epsilon_{t-1} - \cdots - \theta_p\epsilon_{t-p}
$$
where $\theta_1, \ldots, \theta_p$ are \emph{arbitrary} constants and $\epsilon_{t-i}$
are white noise. Note that $MA$ processes depend on past residuals, not values.

'Moving average' is a rather misleading name. For the constants $\theta_1, \ldots, \theta_p$
need not be positive and sum up to $1$.
  
## Partial correlation - 1
Sometimes a strong correlation between two random variables $X$ and $Y$ is because
both of them depend on a common random variable $Z$. One may want to remove the
effect of $Z$ to find out the intrinsic correlation between $X$ and $Y$. Partial
correlation is the way to get it. 

Let us first illustrate the effect by means of an example.
\tiny
```{r echo=TRUE}
Z <- seq(1, 10)
X <- 2*Z + 0.5 * rnorm(10)
Y <- -3*Z + 0.75 * rnorm(10)
```
\normalsize
In this example $X$ and $Y$ are strongly correlated with $Z$. But if we did not
know this fact and if we calculate the correlation between them, we get
\tiny
```{r}
cor(X, Y)
```
\normalsize

## Partial correlation - 2
In order to find partial correlation between $X$ and $Y$ we remove the effect 
of $Z$. This is done by regressing each on of them against $Z$ and removing the 
fitted model. Equivalently, the partial correlation is the correlation between
the residuals of the two regressions.
\tiny
```{r echo=TRUE}
X_model <- lm(X ~ Z)
Y_model <- lm(Y ~ Z)
cor(X_model$residuals, Y_model$residuals)
```
\normalsize
Thus, the partial correlation between the two variables is quite small.

## Partial auto-correlation function - 1
In the case of time series $x_t$, we can find the correlation between $x_t$
and $x_{t-h}$ ignoring the effect of the intervening values. Let $\hat{x}_t$
be the regression estimate of $x_t$ using $x_{t+1}, \ldots, x_{t+h-1}$ as
predictors. Let $\hat{x}_{t-h}$ be the regression estimate of $x_{t_h-1}$
using the \emph{same} predictors. Then the partial autocorrelation function
(PACF) is defined as
$$
\Phi_x(t, t-h) = \frac{E[(x_t - \hat{x}_t)(x_{t-h} - \hat{x}_{t-h})]}{\sigma_x(t)\sigma_x(t-h)}
$$

Why do we need PACF? To find out the lagged value that has the strongest 
correlation with the current value. It gives the $p$ in $AR(p)$ model.

## Partial auto-correlation function - 2
The PACF of the time series of number of fish shows that the partial 
autocorrelation function is high only for the first and the second
lags.
```{r}
pacf(rec, main = "# fish")
```

## $ARMA(p, q)$ models.
Recall, that an $AR(p)$ model is
$$
x_t = \beta_0 + \beta_1 x_{t-1} + \cdots + \beta_p x_{t-p} + \epsilon_t.
$$
while an $MA(q)$ model is
$$
x_t = \mu + \epsilon_t - \theta_1\epsilon_{t-1} - \cdots - \theta_q\epsilon_{t-q}
$$
We can combine the two to get $ARMA(p, q)$ model defined by
$$
x_t = \beta_0 + \beta_1 x_{t-1} + \cdots + \beta_p x_{t-p} + \epsilon_t - \theta_1\epsilon_{t-1} - \cdots - \theta_q\epsilon_{t-q}
$$
Among all the models that can be fitted to a data set, we choose the one with 
the least number of parameters. Sometimes, an $ARMA(p, q)$ model might be more
parsimonious than an individual $AR$ or $MA$ models.

## The difference operator $\Delta$ and ARIMA
The difference operator is defined as
$$
\Delta x_t = x_t - x_{t-1}
$$
It is easy to check that
$$
\Delta^2 x_t = \Delta(x_t - x_{t-1}) = x_t - 2x_{t-1} + x_{t-2}
$$
Higher order difference operators $\Delta^d, d \in \boldsymbol{N}$ are defined 
similarly.

Sometimes, the time series $x_t$ may not be stationary but $\Delta^d x_t$
could be. If we can fit an $ARMA(p, q)$ model to $\Delta^d x_t$ then we way
that an $ARIMA(p, d, q)$ model fits $x_t$. ARIMA stands for **A**uto-**R**egressive
**I**ntegrated **M**oving **A**verage. 

If $\Delta^d x_t$ is the $d$-th difference of $x_t$ then $x_t$ is the $d$-th
integral of $\Delta^d x_t$. Hence the word **I**ntegrated in ARIMA.